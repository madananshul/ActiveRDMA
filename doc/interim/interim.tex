\documentclass[10pt]{article}
\usepackage{fullpage}
\usepackage[top=1in,bottom=1in,left=1in,right=1in]{geometry}

\title{15-712 Project Interim Proposal: Active RDMA}
\author{Chris Fallin, Anshul Madan, Filipe Milit\~{a}o}
\date{}
\begin{document}

\maketitle

\section{Introduction}

Traditional distributed services have built upon RPC or RPC-like
interfaces at the network level. In these systems, a server exports a
static set of operations, typically at a high level. For example, a
file server allows a client to open a file in one call, and read a
block of data in the next. An alternate paradigm exists: RDMA (Remote
Direct Memory Access), first proposed in~\cite{thekkath94}, exposes
data structures in the server's memory directly. Client applications
can then, with knowledge of the appropriate structures, implement most
intelligence on the client side and access shared data remotely. The
advantage of this scheme is that it allows low-latency operation if
the RDMA mechanism is implemented efficiently (i.e., in hardware or at
the low level in a system software trap). It also allows more
flexibility in client operations by allowing a finer granularity than
a static high-level API, although this aspect was not evaluated in the
original proposal. The disadvantage is that, for complex
data-structure manipulations or pathological cases (e.g., a long
linked list in memory), the cost of multiple round-trips over the
network can be extreme compared to performing the computation locally
at the server.

We attempt to retain the benefits of both designs by building a
hybrid. In the spirit of Active Networks~\cite{AN-survey}, we propose
to allow clients to send code to the server to execute directly
against the shared data structures. We call this \textbf{Active
  RDMA}. This functionality will allow for the \emph{flexibility} and
\emph{client-driven intelligence} of RDMA while providing the
\emph{data locality} advantages of traditional server-side code.

In order to narrow the scope of our project, we focus specifically on
environments in which security is not an explicit concern. This seems
strange at first in the context of a network-centric proposal, but
such environments do exist. For example, large computing clusters or
datacenters with a single owner, in which all machines or network
devices are trusted and performance is the paramount concern, are one
possible application domain.

Furthermore, the assumption that security is externally provided
(whole-cluster access control) or otherwise an orthogonal problem
allows us to push Active RDMA's code execution to a lower level,
bypassing layers for higher performance. We plan to define our
interface to be sufficiently low-level that it could hypothetically
execute directly on a network interface card, interacting with shared
memory via DMA. This would allow for extremely low latency.

A secondary benefit to this, and one that we explore, lies in the
flexibility of executing arbitrary code segments on the server,
defined by an application. For example, a ``grep''-like operation to
search through some shared data structure is significantly faster when
executed on the server than when querying a fixed interface from the
client side.

Our work consists of several main contributions:

\begin{itemize} \itemsep -2pt

\item Defining a low-level interface for remote code based on Java
  class-file shipping and word-level shared memory access; defining a
  protocol and building reference implementations of a server and a
  client library to enable use of this ActiveRDMA interface.

\item Building a simulation infrastructure to examine the performance
  tradeoffs of the performance split. We base this infrastructure on a
  conglomeration of the Bochs x86 simulator and the Kaffe open-source
  JVM with a timing model, in order to give some level of fidelity to
  our comparison results.

\item Building a network filesystem on top of this infrastructure,
  keeping data in-memory for simplicity, and evaluating against
  existing network filesystems both on traditional workloads and on
  those with application-specific remote code optimizations. We will
  use FUSE on the client side and expose full Unix semantics in order
  to make comparison with traditional network filesystems possible.

\end{itemize}

\section{Background and Related Work}

As discussed above, the primary advantage of a basic RDMA system is
that there is low latency for common data-transfer operations that
require little actual logic on the server side, and deal with simple
data structures. However, the inability to execute code on the server
can lead to unacceptably high cost for some common operations, such as
a linked list traversal. The proposal in~\cite{thekkath94} gets around
this limitation by also allowing RPC in some cases (e.g., when their
nameserver implementation cannot find an entry after a certain number
of hash-table probes). This is a win for performance, but then
eliminates the flexibility that client-side logic has for
application-specific optimizations. Our Active RDMA proposal retains
this flexibility while attempting to regain some of the server-side
performance.

Our work differs in several ways from previous work and other
alternatives:

\begin{itemize} \itemsep -2pt

\item Active Networks are primarily proposed for customization at the
  network or routing layer: for example, upgrading TCP~\cite{AN-tcp},
  or providing programmable packet-forwarding as in ANTS~\cite{ANTS},
  PLANet~\cite{planet} or FIRE~\cite{FIRE}.

\item Active Disks~\cite{AD2,AD} execute code at a disk controller,
  increasing data locality for certain types of data preprocessing in
  the same way as Active RDMA will allow for storage
  applications. However, Active Disks are limited in scope to this
  data processing. In contrast, Active RDMA is a generic mechanism
  that not only will allow for storage applications but any
  distributed system that requires some shared state at a central
  server.

\item Mobile code~\cite{mobile} is the name for a general research
  thrust of code migration over networks in order to build distributed
  systems. This is the most similar to our proposal. However, mobile
  code is generally a higher-level abstraction that does not provide
  direct access to a shared memory segment, for example, and provides
  more sandboxing and security that remove many of the potential
  performance benefits.

\end{itemize}

\section{Evaluation Section}

\begin{itemize}
\item Basic performance: sequential read, write, Modified Andrew Benchmark
\begin{itemize}
\item Compare against NFS, AFS
\item Warm the pagecache for NFS, AFS comparisons to make in-memory
  comparison fair
\end{itemize}
\item Scaling performance: read/write bandwidth as number of clients grows
\item Performance as JVM speed varies: sweep the JVM speed relative to
  the main CPU speed
\item Performance as DMA cost varies: sweep the cost of memory access
  from the NIC
\item Performance as network latency varies: sweep the round-trip time
\item Custom code-assisted applications (compare against NFS, AFS)
\begin{itemize}
\item Remote grep, remote find
\item Large-tree file copy
\end{itemize}
\end{itemize}

As an important note, in order to achieve accurate timing results, the
benchmark clients (which report execution times) must be run in the
same ``virtual time'' realm as the Bochs-simulated
server. Importantly, wall-clock time taken by Bochs+JVM for a given
operation does not correspond to the counted cycles for that
operation. We will solve this issue one of two ways, depending on
implementation feasibility: either run the clients on the same or
another Bochs instance, clock-synchronized to the server, or else
somehow query ``server time'' before and after each benchmark run
(through a Bochs backdoor, for example) and report that number. The
latter option is much simpler, and only has the potential inaccuracy
that clients appear much faster than they would if everything were
running in the same clock domain; but as long as all filesystems are
evaluated this way, it is still fair in some sense. As a side-benefit,
such an evaluation will minimize the impact of our userspace
(FUSE-based) filesystem client that will be used by the benchmarks.

\bibliography{interim} \bibliographystyle{abbrv}

\end{document}
